# EEG Classification and Analysis: EEGNet and Foundation Models

This repository contains my work for the Fall 2026 PhD Recruitment Task. The project focuses on the implementation, validation, and interpretation of deep learning models for EEG-based Brain-Computer Interfaces (BCIs).

---

## Table of Contents

1.  [Project Overview](#project-overview)
2.  [Project Structure](#project-structure)
3.  [Setup and Installation](#setup-and-installation)
4.  [How to Run](#how-to-run)
5.  [Task 1 Results & Analysis](#task-1-results--analysis)
    *   [Performance Benchmarks](#performance-benchmarks)
    *   [Model Interpretability Visualizations](#model-interpretability-visualizations)
6.  [Next Steps](#next-steps)

---

## Project Overview

The project is divided into three main tasks:

*   **Task 1: EEGNet Implementation & Analysis**
    *   Implement the EEGNet-v4 model from scratch in PyTorch.
    *   Build a robust data loading and preprocessing pipeline for standard BCI datasets (`BCI-IV-2a`, `BCI-IV-2b`, `PhysioNet-MI`).
    *   Validate the model using a "Leave-One-Subject-Out" cross-validation scheme and reproduce benchmark results from the original paper.
    *   Perform in-depth interpretability analysis by visualizing learned filters (spatial and temporal) and feature separability (multi-stage t-SNE).

*   **Task 2: Transformer-based Foundation Model (BIOT)**
    *    Implement the BIOT model.
    *    Compare training from scratch vs. fine-tuning with pre-trained weights.
    *    Analyze the model's self-attention maps.

---

## Project Structure

The repository is organized as follows:

```
.
├── analysis/
│   ├── generate_all_visualizations.py    # Generates spatial/temporal filter plots
│   └── generate_all_multistage_tsne.py # Generates 3-stage t-SNE plots
│
├── data/
│   ├── load_data.py                      # Handles downloading and loading of all datasets
│   └── preprocessing.py                  # EEGPreprocessor class
│
├── experiments/
│   └── task1_eegnet/                     # Root directory for saved models and logs
│       ├── bci_iv_2a/
│       │   ├── fold_1_best_model.pth     # Example of a saved model
│       │   └── logs/                     # TensorBoard logs for this fold
│       └── ...                           # Other datasets and folds
│
├── models/
│   └── eegnet.py                         # EEGNet model class definition
│
├── model/
│   └── train_eegnet.py                   # Main training script
│
└── README.md                             # This file
```

---

## Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/TyroneZeka/EEG-and-Foundational-Models.git
    cd EEG-and-Foundational-Models
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv env
    source env/bin/activate  # On Windows, use `env\Scripts\activate`
    ```

3.  **Install the required dependencies:**
    *(Note: Creating a `requirements.txt` file from your environment with `pip freeze > requirements.txt` is a best practice.)*
    ```bash
    pip install torch numpy pandas scikit-learn matplotlib mne tensorboard tqdm
    ```

---

## How to Run

### 1. Train the EEGNet Model

The main training script will run the "Leave-One-Subject-Out" cross-validation for all specified datasets.

```bash
python model/train_eegnet.py
```

### 2. Monitor Training with TensorBoard

While the model is training, you can view live metrics (loss, accuracy) and gradient distributions.

```bash
# In a new terminal, from the project root directory
tensorboard --logdir experiments/
```
Navigate to `http://localhost:6006/` in your browser.

### 3. Generate All Visualizations

After training is complete and models are saved, run the analysis scripts to generate all figures for the report.

*   **Generate Spatial and Temporal Filter Plots:**
    ```bash
    python analysis/generate_all_visualizations.py
    ```

*   **Generate Multi-Stage t-SNE Plots:**
    ```bash
    python analysis/generate_all_multistage_tsne.py
    ```
The figures will be saved in the `analysis/final_figures/` and `analysis/multistage_tsne/` directories, respectively.

---

## Task 1 Results & Analysis

### Performance Benchmarks

The model performance was validated against the original paper's benchmarks, confirming a successful reproduction.

| Dataset | Our Implementation (Mean Accuracy) | Paper Benchmark (Table 5) | Status |
| :--- | :--- | :--- | :--- |
| **BCI_IV_2a** | 44.0% | 43.3% | **Successfully Reproduced** |
| **BCI_IV_2b** | 66.3% | 66.1% | **Successfully Reproduced** |
| **PhysioNet_MI** | 33.2% | 36.1% | **Closely Aligned** |

### Model Interpretability Visualizations

#### Multi-Stage t-SNE Analysis

These plots demonstrate the model's ability to learn a separable feature space, progressing from raw data to hidden features to final output logits.

*(Example of how an image is embedded. The paths below point to figures generated by the analysis scripts.)*
**`BCI_IV_2a` - Fold 1**
| Stage 1: Raw Input | Stage 2: Hidden Features | Stage 3: Output Logits |
| :---: | :---: | :---: |
| ![Raw t-SNE](figures/stage1_raw_input.png) | ![Hidden t-SNE](figures/stage2_hidden_features.png) | ![Logits t-SNE](figures/stage3_output_logits.png) |

#### Learned Filters (Spatial & Temporal)

These plots confirm that the model learns neurophysiologically relevant patterns. The spatial filters focus on motor cortex regions, and the temporal filters are sensitive to Alpha/Beta frequency bands.

![Learned Filters for BCI_IV_2a](figures/spatial_filters.png)


---

## Next Steps

-   [ ] **Task 2:** Begin implementation of the BIOT foundation model.
-   [ ] **Task 3:** Start literature review for paper selection.

```